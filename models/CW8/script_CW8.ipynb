{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import statsmodels.api as sma\n",
    "import statsmodels.stats.outliers_influence  as smo\n",
    "from scipy.stats import uniform, randint\n",
    "from model_mngr import *\n",
    "import model_mngr\n",
    "from split_merge import *\n",
    "import split_merge\n",
    "from add_indicators import *\n",
    "import add_indicators\n",
    "from maria_import_export import *\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from boruta import BorutaPy\n",
    "import maria_import_export\n",
    "reload(maria_import_export)\n",
    "reload(add_indicators)\n",
    "reload(split_merge)\n",
    "reload(model_mngr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for CW8 ETF from Maria\n",
    "Add indicators and labels and save the dataset on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"../../../Data/DTS_FULL/\"\n",
    "PATH_MODEL = \"../../../Data/Models/\"\n",
    "symb = \"CW8\"\n",
    "dts_name = \"DCA_CLOSE_1D_V1\"\n",
    "rnd_key = int(get_conf(\"RANDOM_KEY\"))\n",
    "df = pd.DataFrame()\n",
    "if \"con\" in locals():\n",
    "    close_connection(con)\n",
    "con = get_connection()\n",
    "df = get_candles_to_df(con=con, symbol=symb, only_close=True)\n",
    "df = add_indicators_to_df(con=con, df_in=df, dts_name=dts_name)\n",
    "df.sort_index(inplace=True)\n",
    "#df.round(5).to_csv(PATH_DATA+dts_name+\"_full.zip\", sep=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the dataset droping useless features\n",
    "Split the dataset by labels, train, val, conf part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_indicators_by_type(\n",
    "    con=con, df_in=df, dts_name=dts_name, symbol=symb, ind_type=0)\n",
    "list_label = get_ind_list_by_type_for_dts(\n",
    "    con=con, dts_name=dts_name, symbol=symb, ind_type=2)\n",
    "dict_split = split_df_by_label_strat(\n",
    "    df_in=df, list_label=list_label['LABEL'].tolist(), split_timeframe=\"Q\")\n",
    "print(list_label)\n",
    "lab = list_label['LABEL'][3]\n",
    "df_test = dict_split['df_'+lab+'_train']\n",
    "df_test.sort_index(inplace=True)\n",
    "print(df_test.loc[:, lab].dropna().iloc[[0, -1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selection of the df studied and plot data to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_split.keys()\n",
    "# lab_studied=\"lab_perf_21d\"\n",
    "lab_studied = \"lab_perf_21d\"\n",
    "df_studied = \"df_\"+lab_studied\n",
    "df_selected = dict_split[df_studied+'_train']\n",
    "df_valid = dict_split[df_studied+'_valid']\n",
    "df_confirm = dict_split[df_studied+'_confirm']\n",
    "list_feat = get_ind_list_by_type_for_dts(\n",
    "    con=con, dts_name=dts_name, symbol=symb, ind_type=1)\n",
    "list_feat = list_feat['LABEL'].tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data for each  label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in list_label[\"LABEL\"].to_list():\n",
    "    print(f\"label : \"+lab)\n",
    "    print(df.loc[:, lab].dropna().iloc[[0, -1]])\n",
    "(df.loc[:, list_label[\"LABEL\"]]).hist(figsize=(8, 10))\n",
    "# df_selected.hist(figsize=(15,20))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split dataframe into X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train, col_y_train = split_df_x_y(\n",
    "    df_in=df_selected, list_features=list_feat, str_label=lab_studied, drop_na=True)\n",
    "df_x_valid, col_y_valid = split_df_x_y(\n",
    "    df_in=df_valid, list_features=list_feat, str_label=lab_studied, drop_na=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of train set, heatmap correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train = df_x_train.corr()\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "corr_train.replace(1,0,inplace=True)\n",
    "corr_train=corr_train.applymap(lambda x : None if x< 0.9 and x>-0.9 else x)\n",
    "corr_train.dropna(axis=0,how='all',inplace=True)\n",
    "corr_train.dropna(axis=1,how='all',inplace=True)\n",
    "sns.heatmap(corr_train, annot=False, cmap='coolwarm', vmin=-1, vmax=1, ax=ax)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train_VIF = sma.add_constant(df_x_train)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"]=df_x_train_VIF.columns\n",
    "vif[\"VIF\"]= [smo.variance_inflation_factor(df_x_train_VIF.values, i) for i in range(df_x_train_VIF.shape[1])]\n",
    "print(vif)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature analysis with Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bo = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "boruta_selector = BorutaPy(rf_bo, n_estimators='auto',\n",
    "                           verbose=1, random_state=int(rnd_key))\n",
    "boruta_selector.fit(df_x_train.values, col_y_train.values)\n",
    "selected_features = df_x_train.columns[boruta_selector.support_]\n",
    "print(selected_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning for Random Forest with random search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees in the foreset\n",
    "n_estimators = [int(x) for x in np.linspace(start=50, stop=200, num=12)]\n",
    "# max number of features considered for splitting a node\n",
    "max_features = ['sqrt']\n",
    "# max number of levels in each decision tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 6, num=5)]\n",
    "# min number of data points placed in a node before the node is split\n",
    "min_samples_split = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "# min number of data points allowed in a leaf node\n",
    "min_samples_leaf = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_fitted = randomizedsearch_cv_fit_report(estimator=rf, param_distributions=random_grid,\n",
    "                                           x_train=df_x_train, y_train=col_y_train, random_state=int(rnd_key), n_iter=200, n_top=5)\n",
    "\n",
    "# 21 : n_estimators': 90, 'min_samples_split': 0.1, 'min_samples_leaf': 0.1, 'max_features': 'sqrt', 'max_depth': 6\n",
    "# 62 : 'n_estimators': 172, 'min_samples_split': 0.5, 'min_samples_leaf': 0.2, 'max_features': 'sqrt', 'max_depth': 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check parameters and test with validation dataset and show feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_frst_reg = RandomForestRegressor(n_estimators=172, bootstrap=True, criterion=\"squared_error\",\n",
    "                                      random_state=rnd_key, max_depth=5, min_samples_split=0.5, min_samples_leaf=0.2, max_features=\"sqrt\")\n",
    "rand_frst_reg.fit(df_x_train, col_y_train)\n",
    "\n",
    "\n",
    "print(f\"Training score : {rand_frst_reg.score(df_x_train, col_y_train)}\")\n",
    "\n",
    "y_pred_valid = rand_frst_reg.predict(df_x_valid)\n",
    "print(\n",
    "    f\"Random Forest regressor score:{rand_frst_reg.score(df_x_valid,col_y_valid)}\")\n",
    "\n",
    "feat_imp = rand_frst_reg.feature_importances_\n",
    "std_feat_imp = np.std(\n",
    "    [tree.feature_importances_ for tree in rand_frst_reg.estimators_], axis=0)\n",
    "pd.Series(feat_imp, index=df_x_train.columns).sort_values(ascending=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_pkl = \".pkl\"\n",
    "rf_suffix = \"_rf_reg_v1\"\n",
    "joblib.dump(rand_frst_reg, PATH_MODEL+symb+\"_\"+lab_studied+rf_suffix+ext_pkl)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing with XGBoost\n",
    "Hyperparameter tuning for XGBoost with random search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5 min 30 s !!\n",
    "\n",
    "random_grid = {\n",
    "    # default 1 subsample ratio of columns when constructing each tree\n",
    "    \"colsample_bytree\": [x for x in np.linspace(0.5, 1, num=6)],\n",
    "    # default 0 minimum loss reduction to make a further patition on a leaf\n",
    "    \"gamma\": [x for x in np.linspace(0, 0.1, num=3)],\n",
    "    \"learning_rate\": [x for x in np.linspace(0.05, 0.2, num=8)],  # default 0.1\n",
    "    # default 6 max depth of the tree\n",
    "    \"max_depth\": [int(x) for x in np.linspace(2, 8, num=5)],\n",
    "    # default 100\n",
    "    \"n_estimators\": [int(x) for x in np.linspace(start=50, stop=200, num=7)],\n",
    "    # default 1 subsample ratio of the training instances\n",
    "    \"subsample\": [x for x in np.linspace(0.4, 1, num=6)],\n",
    "    # default 1 minimum sum of instance weight needed in a child\n",
    "    \"min_child_weight\": [x for x in np.linspace(0.8, 2, num=7)]\n",
    "}\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "\n",
    "xgb_fitted = randomizedsearch_cv_fit_report(estimator=xgb_reg, param_distributions=random_grid,\n",
    "                                            x_train=df_x_train, y_train=col_y_train, random_state=int(rnd_key), n_iter=200, n_top=5)\n",
    "\n",
    "# 21 Parameters: {'subsample': 1.0, 'n_estimators': 125, 'min_child_weight': 1.0, 'max_depth': 2, 'learning_rate': 0.1142857142857143, 'gamma': 0.1, 'colsample_bytree': 0.7}\n",
    "# 21 {'subsample': 1.0, 'n_estimators': 75, 'min_child_weight': 1.4, 'max_depth': 6, 'learning_rate': 0.15714285714285717, 'gamma': 0.1, 'colsample_bytree': 0.6}\n",
    "# 62 'subsample': 1.0, 'n_estimators': 125, 'min_child_weight': 1.4, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.05, 'colsample_bytree': 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoir un gamma <=0.3 voire 0.1 et meme 0\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 1.0,\n",
    "    \"gamma\": 0.05,\n",
    "    \"colsample_bytree\": 1.0,\n",
    "    \"max_depth\": 3,\n",
    "    \"n_estimators\": 125,\n",
    "    \"min_child_weight\": 1.4\n",
    "}\n",
    "\n",
    "# params = {\n",
    "#     \"learning_rate\": 0.11,\n",
    "#     \"subsample\": 1.0,\n",
    "#     \"gamma\": 0.1,\n",
    "#     \"colsample_bytree\":0.7,\n",
    "#     \"max_depth\":2,\n",
    "#     \"n_estimators\":100,\n",
    "#     \"min_child_weight\":1.0\n",
    "# }\n",
    "xgb_reg = xgb.XGBRegressor(random_state=int(rnd_key),\n",
    "                           learning_rate=params['learning_rate'], subsample=params['subsample'], gamma=params['gamma'],\n",
    "                           colsample_bytree=params['colsample_bytree'], max_depth=params['max_depth'],\n",
    "                           n_estimators=params['n_estimators'], min_child_weight=params['min_child_weight'], verbosity=1)\n",
    "\n",
    "# xgb_reg=xgb.XGBRegressor()\n",
    "xgb_reg.fit(df_x_train, col_y_train)\n",
    "\n",
    "print(f\"Training score : {xgb_reg.score(df_x_train, col_y_train)}\")\n",
    "\n",
    "#y_pred_valid = xgb_reg.predict(df_x_valid)\n",
    "\n",
    "print(\n",
    "    f\"Accuracy XGBoost regressor Validation:{xgb_reg.score(df_x_valid,col_y_valid)}\")\n",
    "\n",
    "# print(xgb_reg.feature_importances_)\n",
    "xgb.plot_importance(xgb_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_pkl = \".pkl\"\n",
    "xgb_suffix = \"_xgb_reg_v1\"\n",
    "joblib.dump(xgb_reg, PATH_MODEL+symb+\"_\"+lab_studied+xgb_suffix+ext_pkl)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing with Ridge regression \n",
    "Hyperparameter tuning for Logistic Regression with random search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid = {\n",
    "    \"solver\": [\"svd\", \"cholesky\", \"lsqr\", \"sag\"],\n",
    "    \"alpha\": [x for x in np.logspace(1e-5, 100, 20)],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"normalize\": [True, False]\n",
    "}\n",
    "lin_reg = Ridge()\n",
    "\n",
    "lin_fitted = randomizedsearch_cv_fit_report(estimator=lin_reg, param_distributions=random_grid,\n",
    "                                            x_train=df_x_train, y_train=col_y_train, random_state=int(rnd_key), n_iter=200, n_top=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"solver\": \"cholesky\",\n",
    "    \"normalize\": False,\n",
    "    \"fit_intercept\": True,\n",
    "    \"alpha\": 183302\n",
    "}\n",
    "\n",
    "lin_reg = Ridge(random_state=int(rnd_key),\n",
    "                solver=params['solver'], normalize=params['normalize'], fit_intercept=params['fit_intercept'],\n",
    "                alpha=params['alpha'])\n",
    "\n",
    "lin_reg.fit(df_x_train, col_y_train)\n",
    "\n",
    "print(f\"Training score : {lin_reg.score(df_x_train, col_y_train)}\")\n",
    "\n",
    "print(\n",
    "    f\"Accuracy Ridge regressor Validation:{lin_reg.score(df_x_valid,col_y_valid)}\")\n",
    "\n",
    "print(lin_reg.coef_)\n",
    "\n",
    "\n",
    "plt.bar(df_x_train.columns, lin_reg.coef_)\n",
    "plt.xticks(range(len(df_x_train.columns)), df_x_train.columns, rotation=90)\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Ridge features importance')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "588c4d5d0abde81a8ebfffdc736a46bcad9c5b92be7be13fb4e3cb13087ce002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
