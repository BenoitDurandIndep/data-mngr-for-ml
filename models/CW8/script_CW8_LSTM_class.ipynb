{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from sklearn.model_selection import  TimeSeriesSplit,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import pickle\n",
    "from scipy.stats import uniform, randint,probplot,shapiro\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Bidirectional, Dropout, Activation, Dense, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier as scikit_KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier as scikeras_KerasClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import maria_import_export as maria\n",
    "import model_mngr as modmgr\n",
    "import split_merge as sm\n",
    "import add_indicators as indic\n",
    "import balance\n",
    "reload(maria)\n",
    "reload(indic)\n",
    "reload(sm)\n",
    "reload(modmgr)\n",
    "reload(balance)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for CW8 ETF from Maria\n",
    "Add indicators and labels and save the dataset on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"../../../Data/DTS_FULL/\"\n",
    "PATH_MODEL = \"../../../Data/Models/\"\n",
    "PATH_RES = \"../../../Data/Res/\"\n",
    "symb = \"CW8\"\n",
    "dts_name = \"DCA_CLOSE_1D_V1\"\n",
    "rnd_key = int(maria.get_conf(\"RANDOM_KEY\"))\n",
    "df = pd.DataFrame()\n",
    "if \"con\" in locals():\n",
    "    maria.close_connection(con)\n",
    "con = maria.get_connection()\n",
    "df = maria.get_candles_to_df(con=con, symbol=symb, only_close=True)\n",
    "df = indic.add_indicators_to_df(con=con, df_in=df, dts_name=dts_name)\n",
    "df.sort_index(inplace=True)\n",
    "#df.round(5).to_csv(PATH_DATA+dts_name+\"_full.zip\", sep=\",\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the dataset droping useless features\n",
    "Split the dataset by labels, train, val, conf part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = indic.drop_indicators_by_type(\n",
    "    con=con, df_in=df, dts_name=dts_name, symbol=symb, ind_type=0)\n",
    "list_label = indic.get_ind_list_by_type_for_dts(\n",
    "    con=con, dts_name=dts_name, symbol=symb, ind_type=2)\n",
    "print(list_label)\n",
    "lab_studied = \"lab_perf_21d\"\n",
    "\n",
    "df_clean=sm.remove_columns(df_in=df,df_labels=list_label,str_label=lab_studied)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selection of the df studied and plot data to check\n",
    "!! change lab_studied and algo_studied before training !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_split.keys()\n",
    "algo_studied = \"LSTM_CLASS\"\n",
    "lab_df_studied = \"df_\"+lab_studied\n",
    "seq_len = 20\n",
    "#categ_lab={0:[0.06,100],1:[0.04,0.06],2:[0.015,0.03],3:[0,0.015],4:[-0.03,0],5:[-100,-0.03]}#categ for CW8 lab 21d\n",
    "#categ_lab={0:[0.05,100],1:[0.025,0.05],2:[0.01,0.025],3:[-0.01,0.01],4:[-0.04,-0.01],5:[-100,-0.04]}#categ for CW8 lab 21d\n",
    "categ_lab={0:[0.04,100],1:[0.02,0.04],2:[0.00,0.02],3:[-0.025,0.00],4:[-100.0,-0.025]}#categ for CW8 lab 21d\n",
    "\n",
    "df_class=balance.add_lab_by_class(df_in=df_clean,str_label=lab_studied,categ=categ_lab,bool_replace_label=True)\n",
    "\n",
    "df_selected = indic.drop_indicators_not_selected(con=con, df_in=df_class, dts_name=dts_name, symbol=symb,label=lab_studied,algo=algo_studied)\n",
    "\n",
    "# df_selected.round(5).to_csv(PATH_DATA+dts_name+\"_train_colab_lstm_2307_1.zip\", sep=\",\")\n",
    "\n",
    "# dict_split = sm.split_df_by_label_strat(\n",
    "#     df_in=df_class, list_label=list_label['LABEL'].tolist(), split_timeframe=\"M\",random_split=False,split_strat=(70,15,15))\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, x_conf, y_conf = sm.split_df_strat_lstm(df_in=df_selected,str_label=lab_studied,split_strat=(70,15,15),sequence_length=seq_len)\n",
    "\n",
    "#print(df_test.info())\n",
    "\n",
    "#call function replace lab_studied by lab_studied_class .sort_index(inplace=True)\n",
    "print(f\"SHAPES : {df_class.shape=} {x_train.shape=}  {y_train.shape=}  {x_valid.shape=} {y_valid.shape=} {x_conf.shape=} {y_conf.shape=} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_selected[lab_studied]\n",
    "data.hist(figsize=(8, 3),bins=50,)\n",
    "print(data.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TomekLinks#RandomUnderSampler\n",
    "# df_usamp=balance.class_undersampler(df_in=df_selected,str_label=lab_studied,str_method=\"RandomUnderSampler\",dict_strat={0:34,1:82,2:300,3:300,4:300,5:151})\n",
    "# df_usamp=balance.class_oversampler(df_in=df_usamp,str_label=lab_studied,str_method=\"SMOTE\",dict_strat={0:100,1:200,2:300,3:300,4:300,5:300})\n",
    "#df_usamp.info()\n",
    "# data = df_usamp[lab_studied]\n",
    "# data.hist(figsize=(8, 3),bins=50,)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_selected.describe())\n",
    "(df_selected).hist(figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm,norm_scaler= balance.normalize_df(df_in=df_selected,str_label=lab_studied,tuple_ft_range=(-1,1))\n",
    "#data = df_selected['pos_sma200']\n",
    "#df_norm['pos_sma200'].hist(figsize=(8, 3),bins=50,)\n",
    "print(df_norm.describe())\n",
    "(df_norm).hist(figsize=(12, 10))\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, x_conf, y_conf = sm.split_df_strat_lstm(df_in=df_norm,str_label=lab_studied,split_strat=(70,15,15),sequence_length=seq_len)\n",
    "\n",
    "#print(df_test.info())\n",
    "\n",
    "#call function replace lab_studied by lab_studied_class .sort_index(inplace=True)\n",
    "print(f\"SHAPES : {df_norm.shape=} {x_train.shape=}  {y_train.shape=}  {x_valid.shape=} {y_valid.shape=} {x_conf.shape=} {y_conf.shape=} \")\n",
    "\n",
    "file_name=dts_name+\"_train_colab_lstm_norm_2307_1\"\n",
    "df_norm.round(5).to_csv(PATH_DATA+file_name+\".zip\", sep=\",\")\n",
    "joblib.dump(norm_scaler,filename=PATH_DATA+file_name+\"_scaler.save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[-1]\n",
    "window_size = seq_len\n",
    "dropout = 0.2\n",
    "num_classes = 5\n",
    "\n",
    "# cat_y_train = keras.utils.to_categorical(col_y_train, num_classes)\n",
    "# cat_y_valid = keras.utils.to_categorical(col_y_valid, num_classes)\n",
    "\n",
    "# df_x_train_exp = np.expand_dims(df_x_train, axis=2)\n",
    "# df_x_valid_exp = np.expand_dims(df_x_valid, axis=2)\n",
    "\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(units=128, return_sequences=True,\n",
    "               input_shape=(window_size, input_dim)))\n",
    "model_LSTM.add(Dropout(rate=dropout))\n",
    "model_LSTM.add(Bidirectional(LSTM((window_size * 2), return_sequences=True)))\n",
    "model_LSTM.add(Dropout(rate=dropout))\n",
    "model_LSTM.add(Bidirectional(LSTM(window_size, return_sequences=False)))\n",
    "model_LSTM.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model_LSTM.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_LSTM.fit(x_train, y_train, #batch_size=32,\n",
    "                         shuffle=False, epochs=16, validation_data=(x_valid, y_valid),verbose=0)\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot loss\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING MODEL WITH SCIKIT LEARN\n",
    "input_dim = x_train.shape[-1]\n",
    "window_size = seq_len\n",
    "num_classes = 5\n",
    "rd = 28\n",
    "\n",
    "print(f\"{input_dim=}\")\n",
    "print(\n",
    "    f\"SHAPES : {x_train.shape=}  {y_train.shape=}  {x_valid.shape=} {y_valid.shape=} \")\n",
    "\n",
    "lstm_model = scikit_KerasClassifier(build_fn=modmgr.create_sklearn_lstm_model,\n",
    "                             input_dim=x_train.shape[-1], window_size=seq_len, num_classes=num_classes, layers=[64, seq_len * 2, seq_len], dropout=0.2)\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=2)\n",
    "# mc = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "history = lstm_model.fit(x_train, y_train, epochs=40, validation_data=(\n",
    "    x_valid, y_valid), callbacks=[es], verbose=0)\n",
    "\n",
    "# saved_model = load_model('best_model.h5')\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot loss\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING MODEL WITH SCIKERAS\n",
    "input_dim = x_train.shape[-1]\n",
    "window_size = seq_len\n",
    "num_classes = 5\n",
    "rd = 28\n",
    "epochs=100\n",
    "filename_tmp_model=\"tmp_best_model.h5\"\n",
    "\n",
    "print(f\"{input_dim=}\")\n",
    "print(\n",
    "    f\"SHAPES : {x_train.shape=}  {y_train.shape=}  {x_valid.shape=} {y_valid.shape=} \")\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\", verbose=2)\n",
    "mc = ModelCheckpoint(filename_tmp_model, monitor=\"val_loss\", save_best_only=True)\n",
    "lstm_model = scikeras_KerasClassifier(model=modmgr.create_scikeras_lstm_model, layers=[64, seq_len * 2, seq_len], dropout=0.2, callbacks=[es,mc])\n",
    "\n",
    "history = lstm_model.fit(x_train, y_train, epochs=epochs, validation_data=(\n",
    "    x_valid, y_valid), verbose=0)#/** param à revoir ***/\n",
    "\n",
    "train_loss = history.history_['loss']\n",
    "val_loss = history.history_['val_loss']\n",
    "\n",
    "# Plot loss\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "saved_model = load_model(filename_tmp_model)\n",
    "saved_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "loss, accuracy = saved_model.evaluate(x_valid, y_valid)\n",
    "print(f\"Save model validation Loss: {loss} Accuracy {accuracy}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM/GRID TRAINING MODEL WITH SCIKERAS\n",
    "input_dim = x_train.shape[-1]\n",
    "num_classes = 5\n",
    "rd = 28\n",
    "epochs = 50\n",
    "n_iter = 2\n",
    "filename_tmp_model = \"tmp_best_model.h5\"\n",
    "\n",
    "es = EarlyStopping(monitor=\"accuracy\", patience=3, mode=\"max\", verbose=2)\n",
    "mc = ModelCheckpoint(filename_tmp_model, monitor=\"accuracy\",\n",
    "                     mode=\"max\", save_freq=\"epoch\", save_best_only=True)\n",
    "lstm_model = scikeras_KerasClassifier(model=modmgr.create_scikeras_lstm_model, optimizer=\"adam\",\n",
    "                                      optimizer__lr=0.1, model__layers=[64, seq_len * 2, seq_len], model__dropout=0.2, callbacks=[es, mc], verbose=0)\n",
    "\n",
    "# parameters = {\n",
    "#     'optimizer__lr': [0.05, 0.1],\n",
    "#     'optimizer__momentum': [0.5, 0.7, 0.9],\n",
    "#     'model__layers': [ [input_dim, 32], [input_dim, 16], [input_dim, 32, 16],\n",
    "#                        [32, seq_len * 2], [32,\n",
    "#                                                 seq_len], [32, seq_len * 2, seq_len],\n",
    "#                        [64, seq_len * 2], [64,\n",
    "#                                                 seq_len], [64, seq_len * 2, seq_len],\n",
    "#                        [128, seq_len * 2], [128, seq_len], [128, seq_len * 2, seq_len],],\n",
    "#     'model__dropout': [0.1, 0.2],\n",
    "#     'fit__epochs': [epochs],\n",
    "#     'fit__batch_size':[32,64,128,256]\n",
    "# }\n",
    "\n",
    "parameters = {\n",
    "    'optimizer__lr': [ 0.1],\n",
    "    'optimizer__momentum': [ 0.9],\n",
    "    'model__layers': [ [32, seq_len * 2]],\n",
    "    'model__dropout': [0.1],\n",
    "    'fit__epochs': [epochs],\n",
    "    'fit__batch_size':[32]\n",
    "}\n",
    "\n",
    "# Object X_SearchCV\n",
    "time_split = TimeSeriesSplit(n_splits=3)\n",
    "search = RandomizedSearchCV(estimator=lstm_model, param_distributions=parameters,\n",
    "                                 n_jobs=1, scoring='accuracy', cv=time_split, n_iter=n_iter, random_state=rd, verbose=4,error_score='raise')\n",
    "\n",
    "search_result = search.fit(x_train, y_train)\n",
    "\n",
    "# print(f\"Best Score: {search_result.best_score_}\")\n",
    "# print(f\"Best Params: {search_result.best_params_}\")\n",
    "\n",
    "results_df = pd.DataFrame(search_result.cv_results_)\n",
    "top_10_results = results_df.sort_values('mean_test_score', ascending=False).head(10)\n",
    "\n",
    "for index, row in top_10_results.iterrows():\n",
    "    print(f\"Hyperparameters: {row['params']}\")\n",
    "    print(f\"Mean Score: {row['mean_test_score']}, Std Score: {row['std_test_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test du model sur la validation\n",
    "saved_model = load_model(filename_tmp_model)\n",
    "saved_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "loss, accuracy = saved_model.evaluate(x_valid, y_valid)\n",
    "\n",
    "print(f\"Validation Loss: {loss} Accuracy :{accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_pkl = \".pkl\"\n",
    "suffix = \"_lstm_v1\"\n",
    "joblib.dump(model_LSTM, PATH_MODEL+symb+\"_\"+lab_studied+suffix+ext_pkl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate scores values  for the dataset and save in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_split.keys()\n",
    "algo_studied = \"LSTM_CLASS\"\n",
    "seq_len = 20\n",
    "ext_pkl = \".pkl\"\n",
    "suffix = \"_lstm_v1\"\n",
    "\n",
    "x_eval_lstm, y_eval_lstm,df_predict = sm.prepare_sequences_with_df(df_in=df_selected,str_label=lab_studied,sequence_length=seq_len)\n",
    "\n",
    "model_LSTM_check = joblib.load(PATH_MODEL+symb+\"_\"+lab_studied+suffix+ext_pkl)\n",
    "arr_res = model_LSTM_check.predict(x_eval_lstm)\n",
    "\n",
    "print(f\"{df_predict.shape=} {x_eval_lstm.shape=} {arr_res.shape=} \")\n",
    "\n",
    "df_predict[\"y_eval\"] = np.concatenate([np.array([np.nan] * (df_predict.shape[0] - len(y_eval_lstm))), np.argmax(y_eval_lstm, axis=1)]) \n",
    "df_predict[\"predict\"] = np.concatenate([np.array([np.nan] * (df_predict.shape[0] - len(arr_res))), np.argmax(arr_res, axis=1)]) \n",
    "\n",
    "df_predict.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict.dropna(inplace=True)\n",
    "df_predict.round(5).to_csv(\n",
    "    PATH_RES+dts_name+\"_\"+lab_studied+\"_predict_\"+algo_studied+\".zip\", sep=\",\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARATION FOR BACKTEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_can = maria.get_candles_to_df(con=con, symbol=symb, only_close=False)\n",
    "df_can.sort_index(inplace=True)\n",
    "df_can.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bt=sm.join_dataframes_backtest(df_candle=df_can,df_score=df_prep,str_col_score='predict_ridge')\n",
    "df_bt.describe(include='all')\n",
    "df_bt.round(5).to_csv(\n",
    "     \"../../../Data/backtest/\"+dts_name+\"_\"+lab_studied+\"_predict_BT.zip\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow import keras\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "def get_model(hidden_layer_dim, meta):\n",
    "    # note that meta is a special argument that will be\n",
    "    # handed a dict containing input metadata\n",
    "    n_features_in_ = meta[\"n_features_in_\"]\n",
    "    X_shape_ = meta[\"X_shape_\"]\n",
    "    n_classes_ = meta[\"n_classes_\"]\n",
    "    print(f\"{n_features_in_=}\")\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(n_features_in_, input_shape=X_shape_[1:]))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Dense(hidden_layer_dim))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.Dense(n_classes_))\n",
    "    model.add(keras.layers.Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    hidden_layer_dim=100,\n",
    ")\n",
    "\n",
    "clf.fit(X, y)\n",
    "y_proba = clf.predict_proba(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "588c4d5d0abde81a8ebfffdc736a46bcad9c5b92be7be13fb4e3cb13087ce002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
